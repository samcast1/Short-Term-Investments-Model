{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOuiPQsROFoahSCLiejB7aQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samcast1/Short-Term-Investments-Model/blob/main/notebooks/4.1_sc_sentiment_analysis_refined.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Objective: Create a model based on BERT pre-trained sentiment analysis model that's suited to detect sentiment in airBNB reviews.**\n",
        "\n",
        "**Plan: Train and evaluate model on every review gathered from the webscrape - approximately 890,000 reviews total.**\n",
        "\n",
        "Use ratings as labels and review text as predictors.\n",
        "\n",
        "Colab offers GPUs, but I may need something more substantial - potentially a high-performing Google Cloud VM.\n",
        "\n",
        "This is the script that should get me off the ground. The primary change will be to first concatenate all reviews from each city in one df prior to the train test split."
      ],
      "metadata": {
        "id": "UhgeIgqnO7Gg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install transformers[torch]\n",
        "!pip install accelerate -U\n",
        "!pip install tensorboard\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, BertConfig, TrainerCallback\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "city = 'birmingham-al'\n",
        "file_path = '{city}_reviews_clean.csv'\n",
        "reviews_df = pd.read_csv(file_path)\n",
        "\n",
        "reviews_df = reviews_df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "def map_star_to_label(star_rating):\n",
        "    return star_rating - 1\n",
        "\n",
        "reviews_df['label'] = reviews_df['rating'].apply(map_star_to_label)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    reviews_df['review_text'],\n",
        "    reviews_df['label'],\n",
        "    test_size=0.3,\n",
        "    random_state=42,\n",
        "    stratify=reviews_df['label']\n",
        ")\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "print(f\"Maximum token length: {tokenizer.model_max_length}\")\n",
        "\n",
        "train_encodings = tokenizer(list(X_train), truncation=True, padding=True)\n",
        "test_encodings = tokenizer(list(X_test), truncation=True, padding=True)\n",
        "\n",
        "class SentimentDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = SentimentDataset(train_encodings, y_train.tolist())\n",
        "test_dataset = SentimentDataset(test_encodings, y_test.tolist())\n",
        "\n",
        "\n",
        "config = BertConfig.from_pretrained('bert-base-uncased')\n",
        "\n",
        "config.num_hidden_layers = 12\n",
        "config.num_labels = 5\n",
        "\n",
        "class_counts = reviews_df['label'].value_counts()\n",
        "total_samples = len(reviews_df)\n",
        "class_weights = {i: total_samples / class_counts[i] for i in class_counts.index}\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "class_weights_tensor = torch.tensor([class_weights[i] for i in range(5)], dtype=torch.float).to(device)\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)\n",
        "\n",
        "def forward(input_ids=None, attention_mask=None, token_type_ids=None, labels=None):\n",
        "    outputs = model.bert(\n",
        "        input_ids=input_ids.to(device),\n",
        "        attention_mask=attention_mask.to(device),\n",
        "        token_type_ids=token_type_ids.to(device),\n",
        "    )\n",
        "    sequence_output = outputs[1]\n",
        "    logits = model.classifier(sequence_output)\n",
        "    loss = None\n",
        "    if labels is not None:\n",
        "        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "        loss = loss_fct(logits.view(-1, model.num_labels), labels.view(-1).to(device))\n",
        "    return (loss, logits) if loss is not None else logits\n",
        "\n",
        "model.forward = forward\n",
        "\n",
        "training_args_run1 = TrainingArguments(\n",
        "    output_dir='./results/run1',\n",
        "    num_train_epochs= 5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=16,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.005,\n",
        "    logging_dir='./logs/run1',\n",
        "    logging_steps=10,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    report_to='tensorboard'\n",
        ")\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "    }\n",
        "\n",
        "trainer_run1 = Trainer(\n",
        "    model=model,\n",
        "    args=training_args_run1,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "output = trainer_run1.train()\n",
        "eval_result = trainer_run1.evaluate()\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=./logs/run1"
      ],
      "metadata": {
        "id": "PhEKcUvyPuKV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}